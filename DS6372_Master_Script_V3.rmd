
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(readxl)
library(dplyr)
library(skimr)
library(ggplot2)
library(tidyverse)
library(GGally)
library(epitools)
library(ROCR)
```

## Import Data

```{r echo=FALSE}
#strip white space helps with the formatting of the
df <- read.csv("https://raw.githubusercontent.com/nedeinlein/AppliedStatsProject2/main/data_folder/adult.data.csv", strip.white = TRUE)
#final test set to validate our model on. Don't run as our test set.
val.df <- read.csv("https://raw.githubusercontent.com/nedeinlein/AppliedStatsProject2/main/data_folder/adult.test.csv", strip.white = TRUE)
```

## Create NA values from ? and factor variables
```{r echo=FALSE}
# change  "?" to NA
df[df=="?"]<-NA
val.df[val.df=="?"]<-NA
#check work
sum(colSums(is.na(df)))
#NA columns are workclass(1836), occupation(1843) and native.country(583)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#              Create Factor Variables
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#target column is income. Well call it a hit if income is ">50K" and assigning it a 1 else 0 this will help match the logit probabilites. Also no NA's in income so this should work.
df$income <- factor((ifelse(df$income == ">50K","High","Low")), levels = c("Low", "High"))
#convert the remaining character columns to factors the lazy way.
df[sapply(df,is.character)] <- lapply(df[sapply(df, is.character)], as.factor)
str(df)
str(val.df)
```

# Re-arrange columns of DF so that income is first followed by numeric cols then factor cols. This will help with model syntax later
```{r}
df <- df %>% dplyr::select(income, age, fnlwgt, education.num, capital.gain, capital.loss, hours.per.week, workclass, education, marital.status, occupation, relationship, race, sex, native.country)
```




## Evaluation of NA values continued

# recall: that #occupateion 1843, Workclass 1836

#these to are very close is there some correlation between these two columns such that by removing NA's we remove a particular level of workclass and occupation from the model?

#currently we are just removing them and saying we have enough data no big deal.
```{r}
library(epiR)
#count of NA group
data.frame(sapply(df, function(y) sum(length(which(is.na(y))))))
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#     Before we filter out NA's Lets explore what if any impact this has if any
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
table(df$income)
# un even target variable
# low (24720) 74.91% High (7841) 24.08%
#count the number of rows with NA values
sum(apply(df, 1, anyNA))
# 2399 rows contain NA's
#filter NA
df2 <- df %>% filter(!is.na(workclass) & !is.na(native.country) & !is.na(occupation))
#check of impact of filtering NA
dim(df)
#32561
dim(df2)
#30162   32561 - 2399 = 30162 checks out!
table(df2$income)
# low (22654) 75.1% High (7508) 24.9%
# Not a huge impact on target variable. --- Is that all that Matters??? ---
# one of two largest NA columns
tab.NA.Workclass <- (table(is.na(df$workclass), df$income))
#The odds of a  !NA workclass observation having low income is 0.35 times the odds of a NA class having low income
#inverse: odds of NA worklclass having low income is 2.85 times the odds of a !NAclass
# one of two largest NA columns
tab.NA.Occupation <- (table(is.na(df$occupation), df$income))
epi.2by2(tab.NA.Workclass, method = "cohort.count", conf.level = 0.95)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# I am unsure about the interpreataion of these results. But I think its important to understanding
# our approach to dealing with missing values rather just saying we have a lot of data so whatevs
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
```


#summary statistics
```{r}
#summary stats
summary(df2)
#most data is collected from United States (27504).  all other countries (2658)
table(df2$income)
# un even target variable
# low (22654) High (7508)
#have extreme info for capital.gain and Hours/week
df2[order(-df2$hours.per.week),]
#apparently there are a number of people that work 99 hours a week
# These people tend to be self employed or private employed, Farmers, Truckers, Fishing. Makes sense. 99 hours a week still leaves about 8.5 for sleeping. So its Feasiable. Say for farmers everything you do is part of your job likewise self emplopyed/ Sole proprietorship you could say everything you do is related to the business. Fishing as well. I worked in Alaska as a fisherman can confirm 16 hour days / 7 days a week (when working)  Likely this number was self generated in a questionaire we can look this up.
df2[order(-df2$capital.gain),]
#as well as a number that maxed the capital gain information
#note. all who maxed out capital gain are categorized as our target variable High income (>50K) This makes sense.
summary(df2)
```
Income
Low  High
22654  7508

native.country  
United-States   :27504  
Mexico          :  610    
Philippines     :  188              
Germany         :  128              
Puerto-Rico     :  109              
Canada           :  107              
(Other)           : 1516


Obviously the majority of participants are from the USA in the dataset. Once we filter out USA,
We can see the distribution of other countries
```{r}
library(tm)
library(wordcloud)
wordcloud_df <- read.csv("https://raw.githubusercontent.com/nedeinlein/AppliedStatsProject2/main/data_folder/adult.data.csv", strip.white = TRUE)
make_cloud <- function(txt_vector, txt_filters){
  dtm <- TermDocumentMatrix(Corpus(VectorSource(txt_vector)))
  m <- as.matrix(dtm)
  v <- sort(rowSums(m), decreasing = TRUE)
  d <- data.frame(word = names(v), freq = v) %>% filter(word != txt_filters)
  # Make the cloud
  wordcloud(word = d$word, freq = d$freq, min.freq = 2,
            max.words = 150, random.order = FALSE, rot.per= 0.35, scale=c(2,.5),
            colors = brewer.pal(8, "Dark2"))
}
## Making the Word Cloud without US
make_cloud(wordcloud_df$native.country, txt_filters = c("united-states"))
## Making the Word Cloud without US / Mexico
make_cloud(wordcloud_df$native.country, txt_filters = c("united-states", "mexico")); rm(wordcloud_df)
```

## Plotting correlation matrix and scatter matrix to visualize correlations
```{r}
## looking for correlations with GGally
library(GGally)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Not super useful and takes a long time to run. commenting out until we need plot
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#ggpairs(df2[, 1:7], aes(color = income, alpha = 0.4))
# Correlation Plot
#ggcorr(df2[, 1:7])
```

## Plotting bar graphs for some key variables
```{r}
#education Distribution Hue income (Important)
df2 %>% ggplot(aes(x=reorder(education, education, function(x)length(x)))) + geom_bar(aes(fill = income)) + theme(axis.text.x = element_text(angle = 90)) + labs(x = "Education", title = "Education by Income")
#for Comparison education.num
# as expected it is the same graph!
df2 %>% ggplot(aes(x=reorder(education.num, education.num, function(x)length(x)))) + geom_bar(aes(fill = income)) + theme(axis.text.x = element_text(angle = 90)) + labs(x = "Education.num", title = "Education by Income")
#Occupation distribution Hue Income
df2 %>% ggplot(aes(x=reorder(occupation,occupation, function(x)length(x)))) + geom_bar(aes(fill = income))+ theme(axis.text.x = element_text(angle = 90)) + labs(x = "Occupation Field", title ="Occupation by Income Class")
#workclass dist Hue income. Looks like nearly half of self-empl-inc makes more than 50K
df2 %>% ggplot(aes(x= reorder(workclass, workclass, function(x)length(x)))) + geom_bar(aes(fill= income)) + scale_x_discrete( guide = guide_axis((n.dodge=3)))+ labs(x = "Work Class", title ="Work Class by Income Class")
#race dist by income class
df2 %>% ggplot(aes(x= reorder(race,race,function(x)length(x)))) + geom_bar(aes(fill=income))+ theme(axis.text.x = element_text(angle = 90)) + labs(x= "Race", title = "Race by Income Class")
#sex dis by income class
df2 %>% ggplot(aes(x= reorder(sex,sex, function(x)length(x)))) + geom_bar(aes(fill = income))+ theme(axis.text.x = element_text(angle = 90)) + labs(x= "Sex", title = "Sex by Income Class")
#Native country dist by income class
#Native Country Dist  Hue Income class
df2 %>% ggplot(aes(x= reorder(native.country,native.country, function(x)length(x)))) + geom_bar(aes(fill= income))+ theme(axis.text.x = element_text(angle = 90)) + scale_x_discrete(guide = guide_axis(check.overlap = TRUE)) + labs(x = "Country", title = "Country by Income Class")
#run this graph again but look at United states vs all other countries
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#             make new df with country United-states or Other
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
master_df <- df2
master_df$native.country <- as.character(master_df$native.country)
master_df$native.country <- factor((ifelse(master_df$native.country == 'United-States', 'United-States','Other')), levels = c('Other','United-States'))
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Native Country Dist US vs the world by income class
master_df %>% ggplot(aes(x= reorder(native.country,native.country, function(x)length(x)))) + geom_bar(aes(fill= income))+ theme(axis.text.x = element_text(angle = 90)) + labs(x = "Country", title =  "Income by Country")
# adding new college education feature
master_df$degreeobtained <- ifelse(master_df$education %in% c('Prof-school','Assoc-acdm', 'Assoc-voc','Bachelors','Masters','Doctorate'), 'Yes','No')
# Formal Degree attainment
master_df$formaldegree <- ifelse(master_df$education %in% c('Bachelors','Masters','Doctorate'), 'Yes','No')
# Plot the degree obtained and the formal degree obtained bar charts
master_df %>% ggplot(aes(x=degreeobtained, )) + geom_bar(aes(fill = income)) + theme(axis.text.x = element_text(angle = 90)) + labs(x = "College Degree ?", title = "Income by College Level Education")
master_df %>% ggplot(aes(x=formaldegree, )) + geom_bar(aes(fill = income)) + theme(axis.text.x = element_text(angle = 90)) + labs(x = "College Degree ?", title = "Income by Formal College Degree")
```
#

#write new cleaned Dataset to Github
```{r}
#clean data write to repo
write.csv(master_df,"cleandata.csv")
```

```{r}
#odds Ratio Tables
oddsratio.wald(table(master_df$workclass,master_df$income))
oddsratio.wald(table(master_df$marital.status,master_df$income))
oddsratio.wald(table(master_df$relationship,master_df$income))
oddsratio.wald(table(master_df$race,master_df$income))
oddsratio.wald(table(master_df$sex,master_df$income))
oddsratio.wald(table(master_df$native.country,master_df$income))
oddsratio.wald(table(master_df$degreeobtained,master_df$income))
oddsratio.wald(table(master_df$education,master_df$income))
```

# Function to visualize categorical relationships
```{r}
# Function to plot interactions of two categorical variables and a numeric response
plot_categorical_interactions <- function(df, response, grp, x2){
  mysummary<-function(x){
    result<-c(length(x),mean(x),sd(x),sd(x)/length(x),min(x),max(x),IQR(x))
    names(result)<-c("N","Mean","SD","SE","Min","Max","IQR")
    return(result)
  }
  sumstats<-aggregate(response~grp*x2,data=df,mysummary)
  sumstats<-cbind(sumstats[,1:2],sumstats[,-(1:2)])
  
  ggplot(sumstats,aes(x=x2,y=Mean,group=grp,colour=grp))+
    ylab(deparse(substitute(response)))+
    xlab(deparse(substitute(x2))) +
    geom_line()+
    geom_point()+
    geom_errorbar(aes(ymin=Mean-SE,ymax=Mean+SE),width=.1) +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
  
}

cor(master_df$hours.per.week, as.numeric(master_df$income))
## Plotting interaction plots ##
plot_categorical_interactions(df = master_df, response = master_df$hours.per.week, grp = master_df$income, x2 = master_df$occupation)
plot_categorical_interactions(df = master_df, response = master_df$hours.per.week, grp = master_df$income, x2 = master_df$workclass)
plot_categorical_interactions(df = master_df, response = master_df$hours.per.week, grp = master_df$income, x2 = master_df$native.country)
```
# Function to plot boxplots of variables after removing outliers
```{r}
# Remove outliers from single column
remove_outliers <- function(x, na.rm = TRUE, ...) {
  qnt <- quantile(x, probs=c(.25, .75), na.rm = na.rm, ...)
  H <- 1.5 * IQR(x, na.rm = na.rm)
  y <- x
  y[x < (qnt[1] - H)] <- NA
  y[x > (qnt[2] + H)] <- NA
  y
}
# Remove outliers from entire dataset
remove_outliers_from_df <- function(df){
  cols <- c(colnames(df))
  df[cols] <- sapply(df[cols], remove_outliers)
  return(df)
}
## This will help us see which columns are good candidates for imputation
library(reshape2)
make_plots <- function(df, plottype = "box", histo_var = NULL){
  if (plottype == "box"){
    melted_df <- melt(df)
    ggplot(data = melted_df, aes(x=variable, y = value)) +
      geom_boxplot(color='blue')
  }
  else {
    ggplot(data = df, aes(x=histo_var)) + geom_histogram() + xlab("X-variable")
  }
}
## Running the boxplots of each variable to check variance
# Simply change the vector of column names to get new results out
make_plots(df = na.omit(remove_outliers_from_df(df[c('age', 'education.num')])),
           plottype = "box")
```
# Function to employ in the next steps: Finds the optimal cutoff given a probabilistic model like Logit
```{r}
optimal_cutoff <- function(probabilities, actuals, test_rng = seq(from = 0.4, to = 0.6, by = 0.01)){
  cutoff_vals <- c()
  aucs <- c()
  for (cutoff in test_rng){
    tmp_prob <- probabilities
    tmp_prob[tmp_prob > cutoff] <- 1
    # create prediction object
    aucs <- append(aucs, performance(prediction(tmp_prob, actuals), "auc")@y.values[[1]])
    cutoff_vals <- append(cutoff_vals, cutoff)
  }
  output_df <- data.frame(auc = aucs, cutoff = cutoff_vals)
  return(output_df[which.max(output_df$auc), ]$cutoff)
}
## USAGE ##
# test_over$Prediction[test_over$incomeProbability>  optimal_cutoff(probabilities = test_over$incomeProbability, actuals = test_over$Prediction) ] = 1
```



```{r}
#running simple logs to check for assumptions
library(regclass)
df4 <- master_df %>% dplyr::select(c(income,age,fnlwgt,capital.gain,capital.loss,hours.per.week,workclass,marital.status,
                                     relationship,race,sex,native.country,degreeobtained))
#education and occupation removed due to issues with perfect collinearity
base<-glm(income~.,family="binomial",data=df4)
summary(base)
VIF(base)
#high vif between marital status and relationship
withmarital <- glm(income~age + fnlwgt + capital.gain + capital.loss + hours.per.week + workclass + marital.status + race + sex + native.country + degreeobtained,family="binomial",data=df4)
summary(withmarital)
VIF(withmarital)
withrelationship <- glm(income~age + fnlwgt + capital.gain + capital.loss + hours.per.week + workclass + relationship + race + sex + native.country + degreeobtained,family="binomial",data=df4)
summary(withrelationship)
VIF(withrelationship)
#marital status should be removed as relationship is more statistically significant
df5 <- master_df %>% dplyr::select(c(income,age,fnlwgt,capital.gain,capital.loss,hours.per.week,workclass,relationship,race,sex,native.country,degreeobtained))


# Testing which is better empirically --> Fitting 2 identical models, one with marital and one with relationship
df4$income <- ifelse(df4$income == "Low", 0, 1)
marital_pred <- predict(withmarital, df4)
relationship_pred <- predict(withrelationship, df4)

optimal_cutoff(probabilities = marital_pred, actuals = df4$income)
optimal_cutoff(probabilities = relationship_pred, actuals = df4$income)

marital_pred <- ifelse(marital_pred > optimal_cutoff(probabilities = marital_pred, actuals = df4$income), 1, 0)
relationship_pred <- ifelse(relationship_pred > optimal_cutoff(probabilities = relationship_pred, actuals = df4$income), 1, 0)

print("Marital Model Accuracy and Relationship Model Accuracy, Respectively")
sum(marital_pred == df4$income) / nrow(df4)
sum(relationship_pred == df4$income) / nrow(df4)

# Since we've deemed Relationship to be the better predictor, marital status will be removed from the modelling DF
master_df$marital.status <- NULL

```

```{r}
#testing interaction terms
##adding interaction terms for relationship*age and workclass*capital.gain and capital.gain*capital.loss
inter<-glm(income~age + fnlwgt + capital.loss + hours.per.week + workclass + relationship + race + sex + native.country + degreeobtained + relationship*age ,family="binomial",data=df5)
summary(inter)
VIF(inter)
#those involving capital gain were removed due to issues with perfect collinearity, age relationship stays mostly statistically significant but does not have issues with VIF. Consider keeping
```


```{r}
## PCA should almost always scale data, unless we're positive all variables are on the same scale.
reduced <- master_df[,2:7]
pairs(reduced)
pca.result<-prcomp(reduced,scale.=TRUE)
pca.scores<-pca.result$x
pairs(pca.scores)
par(mfrow=c(1,2))
eigenvals<-(pca.result$sdev)^2
plot(1:6,eigenvals/sum(eigenvals),type="l",main="Scree Plot",ylab="Prop. Var. Explained")
cumulative.prop<-cumsum(eigenvals/sum(eigenvals))
plot(1:6,cumulative.prop,type="l",main="Cumulative proportion",ylim=c(0,1))
#recommend use of PC 1 and 2 if PCA should be use
# PCA helper function
run_pca <- function(df, scale_data = TRUE){
  if(scale_data) {
    # Scale
    df = as.data.frame(scale(df))
  }
  # Run the PCA
  pca <- princomp(df)
  screeplot(pca)
  return(pca$loadings)
}
# Running Scaled PCA
run_pca(reduced, scale = TRUE)

#could add PCs to master df but might fuck up lasso so i'll make another one
df.pca <- data.frame(cbind(master_df, pca.scores))
#scatter plot of PC1 v PC2 Hue income
ggplot(data = df.pca, aes(x=PC1, y=PC2)) + geom_point(aes(col=income)) + ggtitle("PCA of Income")
```

## Clustering Analysis
```{r}
library(cluster)    # clustering algorithms
library(factoextra) #  algos and viz
library(klaR) # Kmodes analysis
# Scale the data and start with a # of clusters
clustering_df <- scale(master_df[, 2:7])
optimal_centers <- 6
# Take results of PCA analysis and enter into this
km_scaled <- kmeans(clustering_df, centers = optimal_centers)
# Get the centers for plotting
km_centers <- data.frame(cluster = factor(1:optimal_centers), km_scaled$centers)
#Visualize the clusters
fviz_cluster(km_scaled, data = clustering_df)
# Calculating ideal # of clusters using elbow method
wss <- function(k){
  kmeans(clustering_df, k, nstart = 10)$tot.withinss
}
# Elbow Method shows 6 is the 'optimal' # of clusters
k.values <- 3:15
wss_values <- map_dbl(k.values, wss)
plot(k.values, wss_values,
     type = "b", pch = 19, frame =FALSE,
     xlab = "Num Clusters of K",
     ylab = "Total WSS")
## Assign the clusters
master_df$cluster <- km_scaled$cluster
```

## Create train test split
```{r}
library(caret)
# 1 is the target variable when income is high aka >50k
master_df$income <- ifelse(master_df$income == "High", 1,0)
master_df$income <- as.factor(master_df$income)
# df2 <- df2 %>% select(-c(education))
test_index <- createDataPartition(master_df$income, p=.3, list = FALSE)
test <- master_df[test_index,]
table(test$income)
train <- master_df[-test_index,]
table(train$income)
prop.table(table(train$income))
```

# No Skill Prediction - No point of a classification model if it cannot outdo a simple majority class prediction
```{r}
noskill <- function(actuals, return_metric = "auc", print_proportion = TRUE){
  majority_class <- (as.data.frame(table(actuals))[as.data.frame(table(actuals))$Freq ==
                                                     max(as.data.frame(table(actuals))$Freq), ])$actuals
  # Print if print is true
  if (print_proportion) {
    print(paste0("Majority class is: ", majority_class,
                 " With ", round(100*(as.data.frame(table(actuals))[as.data.frame(table(actuals))$Freq == max(as.data.frame(table(actuals))$Freq), ])$Freq/sum(as.data.frame(table(actuals))$Freq),2),
                 "% of the sample"))
  }
  # Create the no skill prediction
  noskill_pred <- c(rep(majority_class, length(actuals)))
  # No skill accuracy
  noskill_acc <- sum(noskill_pred == actuals) / length(actuals)
  # No Skill AUC
  noskill_auc <- performance(prediction(as.numeric(noskill_pred), as.numeric(actuals)),
                             "auc")@y.values[[1]]
  # Noskill_F1
  noskill_f1 <- performance(prediction(as.numeric(noskill_pred), as.numeric(actuals)),
                            "f")@y.values[[1]][2]
  
  if(return_metric == "auc"){
    return(noskill_auc)
  }
  else if (return_metric == "f1"){
    return(noskill_f1)
  }
  else{
    return(noskill_acc)
  }
}
# Returning a no skill accuracy
## Return_metric can be Accuracy (acc), AUC (auc) or F1 Score (f1)
noskill_acc <- noskill(actuals = master_df$income, return_metric = "acc", print_proportion = TRUE)
```





## Dealing with imbalanced data sets methods used over sampling, under sampling, mixture of both

```{r}
library(ROSE)
#perform undersampleing. Essentially reduces the number of observations in the majority class income <50k | 0
#calling '$data' extracts the datafram object nested in the ovun.sample object
train_under <- ovun.sample(income ~ ., data = train, method = "under", seed = 1234)$data
test_under <-  ovun.sample(income ~ ., data = test, method = "under", seed = 1234)$data
#perform oversampling. Essentially replicates observations from the minority class. Income >50k | 1
train_over <- ovun.sample(income ~ ., data = train, method = "over", seed = 1234)$data
test_over <-  ovun.sample(income ~ ., data = test, method = "over", seed = 1234)$data
#perform a mix of the two
train_both <- ovun.sample(income ~ ., data = train, method = "both", seed = 1234)$data
test_both <- ovun.sample(income ~ ., data = test, method = "both", seed = 1234)$data
#now we have three data sets to compare results to.
#but which variables should we use?
```

## LOGIT creation function
```{r}
create_logit <- function(f, train, test, return_roc = TRUE){
  # Create the model and prediction column
  mdl <- glm(f, binomial(link = 'logit'), data = train)
  test$p <- predict(mdl, newdata = test, type = "response")
  test['Prediction'] <- 0
  
  # if else statement at optimal cutoff
  test$Prediction[test$p > optimal_cutoff(probabilities = test$p, actuals = test$income)] <- 1
  test$Prediction <- as.factor(test$Prediction)
  
  #confusion matrix: order  predicted classes, Reference
  print(confusionMatrix(test$Prediction, test$income))
  res <- prediction(predict(mdl, newdata = test, type = "response"), test$income)
  roc <- performance(res, measure = "tpr", x.measure = "fpr")
  
  # Return ROC if requested
  if(return_roc) {
    return(roc)
  }
  else{
    return(confusionMatrix(test$Prediction, test$income))
  }
}
f <- as.formula(income ~ education.num + age)
simple_logit_roc <- create_logit(f = f, train = train, test = test, return_roc = TRUE)
cm.simple <- create_logit(f = f, train = train, test = test, return_roc = FALSE)
```

## LASSO creation function
```{r}
library(glmnet)
create_lasso <- function(trainx, trainy, testx, test_df, return_type = "roc"){
  cvfit <- cv.glmnet(trainx, trainy, family = "binomial", type.measure = "class", nlambda = 1000)
  plot(cvfit)
  
  coef<-coef(cvfit,s='lambda.min',exact=TRUE)
  inds<-which(coef!=0)
  variables<-row.names(coef)[inds]
  variables<-variables[!(variables %in% '(Intercept)')]
  
  #CV misclassification error rate is little below .1
  print(paste0("CV Error Rate: ",   cvfit$cvm[which(cvfit$lambda==cvfit$lambda.min)]))
  # Optimal penalty
  print(paste0("Penalty Value: ",   cvfit$lambda.min))
  
  # For final model undersample, refit on test set
  finalmodel<-glmnet(trainx, trainy, family = "binomial",lambda=cvfit$lambda.min)
  
  ## Prediction step
  test_df$p <- predict(finalmodel, newx = testx, type="response")
  # if else statement at optimal cutoff
  test_df["Prediction"] <- 0
  test_df$Prediction[test_df$p > optimal_cutoff(probabilities = test_df$p, actuals = test_df$income)] <- 1
  #turn prediction value into factor
  test_df$Prediction=as.factor(test_df$Prediction)
  
  
  #Confusion matrix: order is -> predicted classes, Reference
  cm <- confusionMatrix(test_df$Prediction, test_df$income)
  res.lasso <- prediction(test_df$p, test_df$income)
  roc.lasso <- performance(res.lasso, measure = "tpr", x.measure = "fpr")
  print(cm)
  
  if(return_type == "roc"){
    return(roc.lasso)
  }
  
  else{
    return(cm)
  }
}
# Get the ROCs
roc.lasso.u = create_lasso(trainx =  model.matrix(income ~ . , train_under)[,-1],
                           trainy = as.factor(train_under[,1]),
                           testx = model.matrix(income ~.,test_under)[,-1],
                           test_df = test_under,
                           return_type = "roc")
roc.lasso.o = create_lasso(trainx =  model.matrix(income ~ . , train_over)[,-1],
                           trainy = as.factor(train_over[,1]),
                           testx = model.matrix(income ~.,test_over)[,-1],
                           test_df = test_over,
                           return_type = "roc")
roc.lasso.b = create_lasso(trainx =  model.matrix(income ~ . , train_both)[,-1],
                           trainy = as.factor(train_both[,1]),
                           testx = model.matrix(income ~.,test_both)[,-1],
                           test_df = test_both,
                           return_type = "roc")
# Get the Confusion Matrices
cm.lasso.u = create_lasso(trainx =  model.matrix(income ~ . , train_under)[,-1],
                          trainy = as.factor(train_under[,1]),
                          testx = model.matrix(income ~.,test_under)[,-1],
                          test_df = test_under,
                          return_type = "cm")
cm.lasso.o = create_lasso(trainx =  model.matrix(income ~ . , train_over)[,-1],
                          trainy = as.factor(train_over[,1]),
                          testx = model.matrix(income ~.,test_over)[,-1],
                          test_df = test_over,
                          return_type = "cm")
cm.lasso.b = create_lasso(trainx =  model.matrix(income ~ . , train_both)[,-1],
                          trainy = as.factor(train_both[,1]),
                          testx = model.matrix(income ~.,test_both)[,-1],
                          test_df = test_both,
                          return_type = "cm")
```


## Decision Tree Model
```{r}
library(rpart)
library(rpart.plot)
# reset train
train_under.DT <- ovun.sample(income ~ ., data = train_under, method = "under", seed = 1234)$data
#duplicate test
test_DT <- test_under
#CP Default is 0.001
DT.Model.u <- rpart(train_under.DT$income ~ ., data = train_under.DT, method = 'class', cp = .001)
#list of Important Variables
Imp.Vars <-varImp(DT.Model.u)
#Print an image of of the decesion tree
rpart.plot(DT.Model.u)
#make Predictions from the DT model
DT.predictions = predict(DT.Model.u, newdata = test_DT[,-1], type = 'class')
#store Confusion Matrix Object of DT.
cm.DT = confusionMatrix(DT.predictions, test_DT$income)
cm.DT
#store  predictions in test_DT
test_DT$Prediction <- DT.predictions
#ROC 
results.dt <- prediction(as.numeric(test_DT$Prediction), as.numeric(test_under$income))
roc.dt = performance(results.dt, measure = "tpr", x.measure = "fpr")
```


## Random Forest Model
```{r}
# Testing a randomforest model!
## WARNING --> With mtry testing, this takes very long to run because it is checking rng*mtry combinations and the randomForest pkg
  ## Does not support multi threading
library(randomForest)
f <- as.formula(income ~ .)
compare_trees <- function(rng, mtry_rng = seq(2,4,1), train, test){
  acc <- c()
  ntree <- c()
  mtry <- c()
  for (i in rng){
    # The standard mtry for classification trees (default) is SQRT(p),
    # Where p is the # of variables in the model
    for (j in mtry_rng){
      rf_regressor = randomForest(f,data = train, ntree = i, mtry = j)
      test$yhat <- predict(rf_regressor, test)
      acc <- append(acc, sum(test$yhat == test$income) / nrow(test))
      ntree <- append(ntree, i)  
      mtry <- append(mtry, j)
    }}
  
  return(data.frame(acc = acc, ntree = ntree, mtry = mtry))
}

# Timing this run for future tests
system.time(trees <- compare_trees(rng = seq(100,150,1),
                                   mtry_rng = seq(3,10,1),
                                   train = train_under,
                                   test = test_under))

# Quick plot of accuracy vs. trees
trees %>% ggplot(aes(x = mtry, y = acc)) + geom_point() + geom_point(aes(col=ntree)) + 
  ggtitle("Plot of Accuracy by Mtry and Ntrees (Color)")

# Train the model
rf_classifier <- randomForest(f, data = train_under,
                              ntree = trees[which.max(trees$acc), ]$ntree,
                              mtry = trees[which.max(trees$acc), ]$mtry)

# Predict and plot the predictions
rf_pred <- predict(rf_classifier, test_under)
# Showing the confusion Matrix
print("Confusion Matrix of Attrition Classification")
confusionMatrix(table(rf_pred, test_under$income))
# Cross validation of our dataset .How does it compare with our one-hoc RMSE?
library(rfUtilities)
rf.crossValidation(x = rf_classifier, xdata = train_under,
                   n = 10, bootstrap = TRUE,
                   seed = 1234)
# RF ROC
results.rf <- prediction(as.numeric(rf_pred), as.numeric(test_under$income))
roc.rf = performance(results.rf, measure = "tpr", x.measure = "fpr")
cm.rf <- confusionMatrix(rf_pred, test_under$income)
```





## Model Comparisons
```{r}
#From sampling schemes only plotting the under method b/c it reflects true values in the data set and because they are all so close
#collectively storing model sensitivities for plotting.
Sensitivities <- c(cm.simple$byClass["Sensitivity"], cm.lasso.u$byClass["Sensitivity"], cm.DT$byClass["Sensitivity"], cm.rf$byClass["Sensitivity"])

Specificities <- c(cm.simple$byClass["Specificity"], cm.lasso.u$byClass["Specificity"], cm.DT$byClass["Specificity"], cm.rf$byClass["Specificity"])

Precisions <- c(cm.simple$byClass["Precision"], cm.lasso.u$byClass["Precision"], cm.DT$byClass["Precision"], cm.rf$byClass["Precision"])

Recalls <- c(cm.simple$byClass["Recall"], cm.lasso.u$byClass["Recall"], cm.DT$byClass["Recall"], cm.rf$byClass["Recall"])

Accuracies <- c(cm.simple$overall[1], cm.lasso.u$overall[1], cm.DT$overall[1], cm.rf$overall[1])

Balanced_Accuracies <- c(cm.simple$byClass["Balanced Accuracy"], cm.lasso.u$byClass["Balanced Accuracy"], cm.DT$byClass["Balanced Accuracy"], cm.rf$byClass["Balanced Accuracy"])


F1_Scores <- c(cm.simple$byClass["F1"], cm.lasso.u$byClass["F1"], cm.DT$byClass["F1"], cm.rf$byClass["F1"])

#create a data frame from metrics in CM and create vector of model names
#model name vector
Models <- c('Simple_LogReg', 'Lasso_LogReg', 'DT', 'Rf')
Casestudy.Results <- data.frame(Models, Sensitivities,Specificities, Precisions, Recalls, Accuracies, Balanced_Accuracies, F1_Scores)
Casestudy.Results
Casestudy.Results$Sensitivities <- round(Casestudy.Results$Sensitivities,digits = 3)
Casestudy.Results$Specificities <- round(Casestudy.Results$Specificities, digits = 3)
Casestudy.Results$Precisions <- round(Casestudy.Results$Precisions, digits  = 3)
Casestudy.Results$Recalls <- round(Casestudy.Results$Recalls, digits = 3)
Casestudy.Results$F1_Scores <- round(Casestudy.Results$F1_Scores, digits=3)
Casestudy.Results$Accuracies <- round(Casestudy.Results$Accuracies, digits= 3)
Casestudy.Results$Balanced_Accuracies <- round(Casestudy.Results$Balanced_Accuracies, digits  = 3)
sn.p <-Casestudy.Results %>% ggplot(aes(Models, Sensitivities, fill = Models)) + geom_bar(stat='identity', alpha = 0.5) +
  geom_text(aes(label=Sensitivities),vjust=3, size = 4) + ggtitle('Comparative Sensitivities') + xlab('') + ylab('')
sp.p <-Casestudy.Results %>% ggplot(aes(Models, Specificities, fill = Models)) + geom_bar(stat='identity', alpha = 0.5) +
  geom_text(aes(label=Specificities),vjust=3, size = 4) + ggtitle('Comparative Specificities') + xlab('') + ylab('')
pr.p <-Casestudy.Results %>% ggplot(aes(Models, Precisions, fill = Models)) + geom_bar(stat='identity', alpha = 0.5) +
  geom_text(aes(label=Precisions),vjust=3, size = 4) + ggtitle('Comparative Precisions') + xlab('') + ylab('')
ba.p <- Casestudy.Results %>% ggplot(aes(Models, Balanced_Accuracies, fill = Models)) + geom_bar(stat='identity', alpha = 0.5) +
  geom_text(aes(label=Balanced_Accuracies),vjust=3, size = 4) + ggtitle('Comparative Balanced_Accuracies') + xlab('') + ylab('')
f1.p <- Casestudy.Results %>% ggplot(aes(Models, F1_Scores, fill = Models)) + geom_bar(stat='identity', alpha = 0.5) +
  geom_text(aes(label=F1_Scores),vjust=3, size = 4) + ggtitle('Comparative F1_Scores') + xlab('') + ylab('')
```
##ROC Curves
```{r}
plot(simple_logit_roc, col = "red")
plot(roc.lasso.u,col ="purple", add = T)
plot(roc.lasso.o, col ="orange", add = T)
plot(roc.lasso.b, col = "blue", add = T)
plot(roc.dt, col = "green", add = T)
plot(roc.rf, col = "black", add = T)
plot(performance(prediction(rep(0,4512), as.numeric(test_under$income)), measure = "tpr", x.measure = "fpr"), col = "yellow", add = T)
legend("bottomright",legend=c("Simple Logit","LASSO Under","LASSO Over","LASSO Mix of both", "DT", "RF", "No Skill"),col=c("red", "purple", "orange", "blue", "green", "black", "yellow"),lty=1,lwd=1)
title("Plot of All Model ROC Curves")
```
